import os
import numpy as np
import tensorflow as tf
# TensorFlow Lite interpreter
from tensorflow.lite.python.interpreter import Interpreter
# Reduce TensorFlow logging
tf.get_logger().setLevel('ERROR')
from absl import logging
logging.set_verbosity(logging.ERROR)
import matplotlib.pyplot as plt

IMAGE_DIR = "dataset/images"
ANNOTATION_DIR = "dataset/annotations"
LABEL_MAP = "dataset/label_map.txt"

TFLITE_MODEL_PATH = "exported_model/road_anomaly_detector.tflite"

BATCH_SIZE = 2  
EPOCHS = 1 
IMG_SIZE = (224, 224)  

os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(ANNOTATION_DIR, exist_ok=True)


with open(LABEL_MAP, 'w') as f:
    f.write("anomaly\n")


for i in range(5):
   
    img = np.random.uniform(0, 1, size=(IMG_SIZE[0], IMG_SIZE[1], 3))
    plt.imsave(os.path.join(IMAGE_DIR, f"img{i}.jpg"), img)
    
  
    if i % 2 == 0:
        with open(os.path.join(ANNOTATION_DIR, f"img{i}.txt"), 'w') as f:
            f.write("0 0.5 0.5 0.5 0.5\n")  # class 0, center, width=0.5, height=0.5


def load_dataset(image_dir, annotation_dir, img_size=(224, 224)):
   
    images = []
    boxes = []
    labels = []
    for img_file in os.listdir(image_dir):
        if not img_file.endswith((".jpg", ".png")):
            continue
        img_path = os.path.join(image_dir, img_file)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)
        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
        images.append(img_array)
        # Load corresponding annotation (YOLO format)
        ann_file = os.path.join(annotation_dir, img_file.replace(".jpg", ".txt").replace(".png", ".txt"))
        box = [0.0, 0.0, 0.0, 0.0]
        label = 0  # background
        if os.path.exists(ann_file):
            with open(ann_file, "r") as f:
                ann_lines = f.readlines()
                if ann_lines:
                    # Take first annotation if multiple
                    parts = list(map(float, ann_lines[0].strip().split()))
                    class_id = int(parts[0]) + 1  # shift for background
                    x_center, y_center, w, h = parts[1:]
                    # Convert YOLO to [ymin, xmin, ymax, xmax] (normalized)
                    ymin = y_center - h / 2
                    xmin = x_center - w / 2
                    ymax = y_center + h / 2
                    xmax = x_center + w / 2
                    box = [ymin, xmin, ymax, xmax]
                    label = class_id
        boxes.append(box)
        labels.append(label)
    return np.array(images, dtype=np.float32), np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int32)

images, boxes, labels = load_dataset(IMAGE_DIR, ANNOTATION_DIR, IMG_SIZE)
print(f"Images shape: {images.shape}, Boxes shape: {boxes.shape}, Labels shape: {labels.shape}")

def create_model(img_size=(224, 224, 3), num_classes=1, alpha=0.35):
    """
    Creates a simple object detection model (Tiny MobileNetV2-based)
    Using smaller alpha for lighter model on Pi
    """
    base_model = tf.keras.applications.MobileNetV2(input_shape=img_size, alpha=alpha, include_top=False, weights=None)  # No pretrained weights, smaller alpha
    base_model.trainable = False
    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(base_model.output)  # Reduced filters
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    
    # Bounding box regression output
    bbox_output = tf.keras.layers.Dense(4, activation='sigmoid', name='bbox')(x)
    
    # Class output
    class_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='class')(x)
    
    model = tf.keras.models.Model(inputs=base_model.input, outputs=[bbox_output, class_output])
    return model

# Determine num_classes: object classes + background
with open(LABEL_MAP) as f:
    num_object_classes = len(f.readlines())
num_classes = num_object_classes + 1  # +1 for background

model = create_model(img_size=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=num_classes, alpha=0.35)
model.summary()

model.compile(optimizer='adam',
              loss={'bbox': 'mse', 'class': 'sparse_categorical_crossentropy'},
              metrics={'bbox': 'mse', 'class': 'accuracy'})

model.fit(x=images, y={'bbox': boxes, 'class': labels},
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

def representative_dataset():
    for image in images:
        yield [np.expand_dims(image, axis=0).astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model = converter.convert()
with open(TFLITE_MODEL_PATH, "wb") as f:
    f.write(tflite_model)
print(f"TFLite model saved at {TFLITE_MODEL_PATH}")

interpreter = Interpreter(model_path=TFLITE_MODEL_PATH)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print("TFLite interpreter ready. Input details:", input_details)
